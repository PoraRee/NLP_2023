{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpCKO19dhHPq"
      },
      "source": [
        "# HW 5 - Neural POS Tagger\n",
        "\n",
        "In this exercise, you are going to build a set of deep learning models on part-of-speech (POS) tagging using Pytorch.\n",
        "\n",
        "To complete this exercise, you will need to build deep learning models for POS tagging in Thai using NECTEC's ORCHID corpus. You will build one model for each of the following type:\n",
        "\n",
        "- Neural POS Tagging (without pretrained weights)\n",
        "- Neural POS Tagging with CRF (with and without pretrained weights)\n",
        "\n",
        "Pretrained word embeddding are already given for you to use (albeit, a very bad one).\n",
        "\n",
        "We also provide the code for data cleaning, preprocessing and some starter code for pytorch in this notebook but feel free to modify those parts to suit your needs. Feel free to use additional libraries (e.g. scikit-learn) as long as you have a model for each type mentioned above.\n",
        "\n",
        "### Don't forget to change hardware accelrator to GPU in runtime on Google Colab ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qx0YbM_8hHPt"
      },
      "source": [
        "## 1. Setup and Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yF3lPNNkhHPu"
      },
      "source": [
        "We use POS data from [ORCHID corpus](https://www.researchgate.net/profile/Virach-Sornlertlamvanich/publication/2630580_Building_a_Thai_part-of-speech_tagged_corpus_ORCHID/links/02e7e514db19a98619000000/Building-a-Thai-part-of-speech-tagged-corpus-ORCHID.pdf), which is a POS corpus for Thai language.\n",
        "A method used to read the corpus into a list of sentences with (word, POS) pairs have been implemented already. The example usage has shown below.\n",
        "We also create a word vector for unknown word by random."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58LvFz30zumq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88a8af63-be38-45cb-f0aa-e79d811f9150"
      },
      "source": [
        "!wget https://www.dropbox.com/s/tuvrbsby4a5axe0/resources.zip\n",
        "!unzip resources.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-18 11:00:34--  https://www.dropbox.com/s/tuvrbsby4a5axe0/resources.zip\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.65.18, 2620:100:6021:18::a27d:4112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.65.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/raw/tuvrbsby4a5axe0/resources.zip [following]\n",
            "--2023-02-18 11:00:35--  https://www.dropbox.com/s/raw/tuvrbsby4a5axe0/resources.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc593c485baf4576364f4c07d8c8.dl.dropboxusercontent.com/cd/0/inline/B2sXxo69D5-Z-181tICIYgC-R3QMu1doPTvSTSL4meK8HP7-O4ISnSbgFudFPoHtdwrImq0nNtCIGSpzxXKZhum-cja7dEJc2-1VSW8OAypWTjFZ7Xjd-nmxhkoFMbOUN7f_vw1QDk-xPeOXx7GSkDkx5kNdRBnXW1CS1_aMkvzGwA/file# [following]\n",
            "--2023-02-18 11:00:35--  https://uc593c485baf4576364f4c07d8c8.dl.dropboxusercontent.com/cd/0/inline/B2sXxo69D5-Z-181tICIYgC-R3QMu1doPTvSTSL4meK8HP7-O4ISnSbgFudFPoHtdwrImq0nNtCIGSpzxXKZhum-cja7dEJc2-1VSW8OAypWTjFZ7Xjd-nmxhkoFMbOUN7f_vw1QDk-xPeOXx7GSkDkx5kNdRBnXW1CS1_aMkvzGwA/file\n",
            "Resolving uc593c485baf4576364f4c07d8c8.dl.dropboxusercontent.com (uc593c485baf4576364f4c07d8c8.dl.dropboxusercontent.com)... 162.125.4.15, 2620:100:6027:15::a27d:480f\n",
            "Connecting to uc593c485baf4576364f4c07d8c8.dl.dropboxusercontent.com (uc593c485baf4576364f4c07d8c8.dl.dropboxusercontent.com)|162.125.4.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/B2sm-BNGxzUllbznT6RAYS1pXhmRkjqKp6s0z6RxmMPz4H_hFfPHwcCaz4Oal-7Wh3UJ0_lukkibwH1SQpGzpiyuU4Yb2lBtFTz7g05WSl7RM_sfL0FFjGmd-65fbF149FKs2bEVvzK8fmWJq3ScHz186UTc_xkL_2lvpr2AhBjQZQC35DYmAjLKlltQ188wuBXJIjSy1TgpjLwvM6UDJ9MS3VegOmEGIGp8YpMsMhL1k8y_cw11FHbanBKuRxj7xjjnv5ZeJni4rSOvl6Hsq6GZ8dn66uiqPdo4gjdEq4jfi2_FV1N2GVDIMIX73XVWvt4N51MBoWAS2FAE-aerdEFmcX0CzkgHj1cZ3hSpi1sYXeyqclgfjrplfVEZ4jV4S_Vdng0uf5MnJKdScoscivSfOu7IQ5MSdFaOO3ZlJ5p4iA/file [following]\n",
            "--2023-02-18 11:00:36--  https://uc593c485baf4576364f4c07d8c8.dl.dropboxusercontent.com/cd/0/inline2/B2sm-BNGxzUllbznT6RAYS1pXhmRkjqKp6s0z6RxmMPz4H_hFfPHwcCaz4Oal-7Wh3UJ0_lukkibwH1SQpGzpiyuU4Yb2lBtFTz7g05WSl7RM_sfL0FFjGmd-65fbF149FKs2bEVvzK8fmWJq3ScHz186UTc_xkL_2lvpr2AhBjQZQC35DYmAjLKlltQ188wuBXJIjSy1TgpjLwvM6UDJ9MS3VegOmEGIGp8YpMsMhL1k8y_cw11FHbanBKuRxj7xjjnv5ZeJni4rSOvl6Hsq6GZ8dn66uiqPdo4gjdEq4jfi2_FV1N2GVDIMIX73XVWvt4N51MBoWAS2FAE-aerdEFmcX0CzkgHj1cZ3hSpi1sYXeyqclgfjrplfVEZ4jV4S_Vdng0uf5MnJKdScoscivSfOu7IQ5MSdFaOO3ZlJ5p4iA/file\n",
            "Reusing existing connection to uc593c485baf4576364f4c07d8c8.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 151484222 (144M) [application/zip]\n",
            "Saving to: ‘resources.zip’\n",
            "\n",
            "resources.zip       100%[===================>] 144.47M  26.4MB/s    in 6.9s    \n",
            "\n",
            "2023-02-18 11:00:43 (20.9 MB/s) - ‘resources.zip’ saved [151484222/151484222]\n",
            "\n",
            "Archive:  resources.zip\n",
            "  inflating: basic_ff_embedding.pt   \n",
            "   creating: data/\n",
            " extracting: data/__init__.py        \n",
            "  inflating: data/__init__.pyc       \n",
            "   creating: data/__pycache__/\n",
            "  inflating: data/__pycache__/__init__.cpython-36.pyc  \n",
            "  inflating: data/__pycache__/orchid_corpus.cpython-36.pyc  \n",
            "  inflating: data/orchid97.txt       \n",
            "  inflating: data/orchid_corpus.py   \n",
            "  inflating: data/orchid_corpus.pyc  \n",
            "  inflating: data/orchid_test.txt    \n",
            "  inflating: data/orchid_train.txt   \n",
            "   creating: embeddings/\n",
            " extracting: embeddings/__init__.py  \n",
            "   creating: embeddings/__pycache__/\n",
            "  inflating: embeddings/__pycache__/__init__.cpython-36.pyc  \n",
            "  inflating: embeddings/__pycache__/emb_reader.cpython-36.pyc  \n",
            "  inflating: embeddings/emb_reader.py  \n",
            "  inflating: embeddings/polyglot-th.pkl  \n",
            "   creating: model/\n",
            "  inflating: model/_DS_Store         \n",
            "  inflating: model/crf_basic.model   \n",
            "  inflating: model/crf_neural.model  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2a9b92hYTg2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7653352-9b3c-4055-ab0e-b73a382e5cc1"
      },
      "source": [
        "!pip install pytorch-crf"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pytorch-crf\n",
            "  Downloading pytorch_crf-0.7.2-py3-none-any.whl (9.5 kB)\n",
            "Installing collected packages: pytorch-crf\n",
            "Successfully installed pytorch-crf-0.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJpCbSvJhHPv"
      },
      "source": [
        "from data.orchid_corpus import get_sentences\n",
        "import numpy as np\n",
        "import numpy.random\n",
        "np.random.seed(42)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "U4SZz56ThHP0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fdf631e-bc37-4cd5-eec8-eced5689db00"
      },
      "source": [
        "yunk_emb =np.random.randn(32)\n",
        "train_data = get_sentences('train')\n",
        "test_data = get_sentences('test')\n",
        "print(train_data[0])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('การ', 'FIXN'), ('ประชุม', 'VACT'), ('ทาง', 'NCMN'), ('วิชาการ', 'NCMN'), ('<space>', 'PUNC'), ('ครั้ง', 'CFQC'), ('ที่ 1', 'DONM')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awxn_GRIhHP3"
      },
      "source": [
        "Next, we load pretrained weight embedding using pickle. The pretrained weight is a dictionary which map a word to its embedding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GS3lTZshHP4"
      },
      "source": [
        "import pickle\n",
        "fp = open('basic_ff_embedding.pt', 'rb')\n",
        "embeddings = pickle.load(fp)\n",
        "fp.close()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CqTNKsChHP7"
      },
      "source": [
        "The given code below generates an indexed dataset(each word is represented by a number) for training and testing data. The index 0 is reserved for padding to help with variable length sequence. (Additionally, You can read more about padding here https://suzyahyah.github.io/pytorch/2019/07/01/DataLoader-Pad-Pack-Sequence.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPGUNEZyhHP8"
      },
      "source": [
        "## 2. Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fMWn8qehHP9"
      },
      "source": [
        "word_to_idx ={}\n",
        "idx_to_word ={}\n",
        "label_to_idx = {}\n",
        "for sentence in train_data:\n",
        "    for word,pos in sentence:\n",
        "        if word not in word_to_idx:\n",
        "            word_to_idx[word] = len(word_to_idx)+1\n",
        "            idx_to_word[word_to_idx[word]] = word\n",
        "        if pos not in label_to_idx:\n",
        "            label_to_idx[pos] = len(label_to_idx)+1\n",
        "word_to_idx['UNK'] = len(word_to_idx)\n",
        "\n",
        "n_classes = len(label_to_idx.keys())+1"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgvZ8v_2hHP_"
      },
      "source": [
        "This section is tweaked a little from the demo, word2features will return word index instead of features, and sent2labels will return a sequence of word indices in the sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktf2KkJghHQA"
      },
      "source": [
        "def word2features(sent, i, emb):\n",
        "    word = sent[i][0]\n",
        "    if word in word_to_idx :\n",
        "        return word_to_idx[word]\n",
        "    else :\n",
        "        return word_to_idx['UNK']\n",
        "\n",
        "def sent2features(sent, emb_dict):\n",
        "    return np.asarray([word2features(sent, i, emb_dict) for i in range(len(sent))])\n",
        "\n",
        "def sent2labels(sent):\n",
        "    return numpy.asarray([label_to_idx[label] for (word, label) in sent],dtype='int32')\n",
        "\n",
        "def sent2tokens(sent):\n",
        "    return [word for (word, label) in sent]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgBw3I9ShHQD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "991b8a43-1ffd-4a65-99df-79f39dcd3225"
      },
      "source": [
        "sent2features(train_data[100], embeddings)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 29, 327,   5, 328])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3O7oClK-hHQG"
      },
      "source": [
        "Next we create train and test dataset, then we use pytorch to post-pad the sequence to max sequence with 0. Our labels are changed to a one-hot vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tJxtPtohHQH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07bc3045-9f39-45c3-b7bc-f28eaae75944"
      },
      "source": [
        "%%time\n",
        "x_train = np.asarray([sent2features(sent, embeddings) for sent in train_data])\n",
        "y_train = [sent2labels(sent) for sent in train_data]\n",
        "\n",
        "x_test =  np.asarray([sent2features(sent, embeddings) for sent in test_data]) \n",
        "y_test = [sent2labels(sent) for sent in test_data]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 228 ms, sys: 4.39 ms, total: 232 ms\n",
            "Wall time: 232 ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<timed exec>:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "<timed exec>:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DG1gvJ4mhHQJ"
      },
      "source": [
        "import torch \n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "x_train = [torch.LongTensor(sentence) for sentence in x_train]\n",
        "y_train = [torch.LongTensor(sentence) for sentence in y_train] \n",
        "x_test = [torch.LongTensor(sentence) for sentence in x_test]\n",
        "\n",
        "x_train = pad_sequence(x_train, batch_first=True)\n",
        "y_train = pad_sequence(y_train, batch_first=True)\n",
        "x_test = pad_sequence(x_test, batch_first=True)\n",
        "\n",
        "maxlen = x_train.size(1)  \n",
        "\n",
        "# Pad the sequence length of x_test to be maxlen \n",
        "remaining_len = x_train.size(1) - x_test.size(1)\n",
        "remaining_mat = torch.zeros((x_test.size(0), remaining_len), dtype=torch.long) \n",
        "x_test = torch.cat((x_test, remaining_mat), dim=1) \n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "from torch.utils.data import Dataset, DataLoader \n",
        "\n",
        "class POSTaggerDataset(Dataset): \n",
        "  def __init__(self, data, labels=None):  \n",
        "    self.data = data \n",
        "    self.labels = labels\n",
        "\n",
        "    if labels is not None: \n",
        "      assert len(data) == len(labels)  \n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if self.labels is None: \n",
        "      return torch.LongTensor(self.data[idx])\n",
        "    else: \n",
        "      return (\n",
        "          torch.LongTensor(self.data[idx]), \n",
        "          torch.LongTensor(self.labels[idx])\n",
        "      )\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "\n",
        "train_dataset = POSTaggerDataset(x_train, y_train) \n",
        "test_dataset = POSTaggerDataset(x_test)\n",
        "\n",
        "num_workers = 2\n",
        "batch_size = 64\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True) \n",
        "test_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False)   "
      ],
      "metadata": {
        "id": "vnOLVCI1pBs6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = next(iter(train_dataloader)) \n",
        "out[0].size(), out[1].size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLKJD4h7X9zB",
        "outputId": "458eb61f-8c4f-49f2-a08d-b4119fddf0b5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([64, 102]), torch.Size([64, 102]))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx_c3-LwhHQP"
      },
      "source": [
        "## 3. Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvCW24orhHQP"
      },
      "source": [
        "Our output from pytorch is a distribution of problabilities on all possible label. outputToLabel will return an indices of maximum problability from output sequence.\n",
        "\n",
        "evaluation_report is the same as in the demo. \n",
        "\n",
        "**Hint** \n",
        "1. ```categorical_accuracy``` is for evaluating training set.\n",
        "2. ```evaluation_report(y_true, y_pred, get_only_acc=True)``` is for evaluating test set since ```y_train``` and ```y_test``` are in different formats."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iqgp2gd4hHQQ"
      },
      "source": [
        "def outputToLabel(yt,seq_len):\n",
        "    out = []\n",
        "    for i in range(0,len(yt)):\n",
        "        if(i==seq_len):\n",
        "            break\n",
        "        out.append(np.argmax(yt[i]))\n",
        "    return out"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzIL_rsAhHQT"
      },
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "# for validation part \n",
        "def categorical_accuracy(preds, y, tag_pad_idx=0):\n",
        "  if len(preds.shape) == 2: \n",
        "    preds = preds.argmax(dim = 1, keepdim = True) # get the index of the max probability\n",
        "    non_pad_elements = (y != tag_pad_idx).nonzero()\n",
        "    correct = preds[non_pad_elements].squeeze(1).eq(y[non_pad_elements]) \n",
        "  else: \n",
        "    non_pad_elements = (y != tag_pad_idx).nonzero()\n",
        "    correct = preds[non_pad_elements].eq(y[non_pad_elements]) \n",
        "  return correct.sum() / y[non_pad_elements].shape[0]\n",
        "\n",
        "def evaluation_report(y_true, y_pred, get_only_acc=False):\n",
        "    # retrieve all tags in y_true\n",
        "    tag_set = set()\n",
        "    for sent in y_true:\n",
        "        for tag in sent:\n",
        "            tag_set.add(tag)\n",
        "    for sent in y_pred:\n",
        "        for tag in sent:\n",
        "            tag_set.add(tag)\n",
        "    tag_list = sorted(list(tag_set))\n",
        "    \n",
        "    # count correct points\n",
        "    tag_info = dict()\n",
        "    for tag in tag_list:\n",
        "        tag_info[tag] = {'correct_tagged': 0, 'y_true': 0, 'y_pred': 0}\n",
        "\n",
        "    all_correct = 0\n",
        "    all_count = sum([len(sent) for sent in y_true])\n",
        "    for sent_true, sent_pred in zip(y_true, y_pred):\n",
        "        for tag_true, tag_pred in zip(sent_true, sent_pred):\n",
        "            if tag_true == tag_pred:\n",
        "                tag_info[tag_true]['correct_tagged'] += 1\n",
        "                all_correct += 1\n",
        "            tag_info[tag_true]['y_true'] += 1\n",
        "            tag_info[tag_pred]['y_pred'] += 1\n",
        "    accuracy = (all_correct / all_count)\n",
        "\n",
        "    # get only accuracy for testing \n",
        "    if get_only_acc: \n",
        "      return accuracy \n",
        "\n",
        "    accuracy *= 100 \n",
        " \n",
        "            \n",
        "    # summarize and make evaluation result\n",
        "    eval_list = list()\n",
        "    for tag in tag_list:\n",
        "        eval_result = dict()\n",
        "        eval_result['tag'] = tag\n",
        "        eval_result['correct_count'] = tag_info[tag]['correct_tagged']\n",
        "        precision = (tag_info[tag]['correct_tagged']/tag_info[tag]['y_pred'])*100 if tag_info[tag]['y_pred'] else '-'\n",
        "        recall = (tag_info[tag]['correct_tagged']/tag_info[tag]['y_true'])*100 if (tag_info[tag]['y_true'] > 0) else 0\n",
        "        eval_result['precision'] = precision\n",
        "        eval_result['recall'] = recall\n",
        "        eval_result['f_score'] = (2*precision*recall)/(precision+recall) if (type(precision) is float and recall > 0) else '-'\n",
        "        \n",
        "        eval_list.append(eval_result)\n",
        "\n",
        "    eval_list.append({'tag': 'accuracy=%.2f' % accuracy, 'correct_count': '', 'precision': '', 'recall': '', 'f_score': ''})\n",
        "    \n",
        "    df = pd.DataFrame.from_dict(eval_list)\n",
        "    df = df[['tag', 'precision', 'recall', 'f_score', 'correct_count']]\n",
        "  \n",
        "    display(df)\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YG-RiHdhHQV"
      },
      "source": [
        "## 4. Train a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U75Ivn2vhHQZ"
      },
      "source": [
        "The model is this section is separated to two groups\n",
        "\n",
        "- Neural POS Tagger (4.1)\n",
        "- Neural CRF POS Tagger (4.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLwL3B7rhHQZ"
      },
      "source": [
        "## 4.1 Neural POS Tagger  (Example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVoB-1XVhHQa"
      },
      "source": [
        "We create a simple Neural POS Tagger as an example for you. This model dosen't use any pretrained word embbeding so it need to use Embedding layer to train the word embedding from scratch."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo "
      ],
      "metadata": {
        "id": "EBPV8QfOr5FS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4aebe6ae-8903-4162-92b2-1cc1477ba634"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.7.2-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchinfo \n",
        "from torch import nn \n",
        "from torch.nn import Embedding, Dropout, GRU, LSTM, Linear, CrossEntropyLoss \n",
        "from torch.optim import Adam\n",
        "\n",
        "\n",
        "class BiGRU(nn.Module): \n",
        "  def __init__(self, word_to_idx):\n",
        "    super(BiGRU, self).__init__() \n",
        "    self.embed = Embedding(len(word_to_idx), 32, padding_idx=0)\n",
        "    self.bi_gru = GRU(32, 32, bidirectional=True, batch_first=True) \n",
        "    self.dropout = Dropout(0.2) \n",
        "    self.classifier = Linear(64, 48) \n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = self.embed(x) \n",
        "    x, _ = self.bi_gru(x) \n",
        "    x = self.dropout(x) \n",
        "    out = self.classifier(x)\n",
        "    return out\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model = BiGRU(word_to_idx) \n",
        "model.to(device) \n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=1e-3) \n",
        "criterion = CrossEntropyLoss(ignore_index=0)\n",
        "print(torchinfo.summary(model))\n",
        "\n",
        "num_epochs = 10 \n",
        "\n",
        "for epoch in range(1, num_epochs+1): \n",
        "  train_losses = [] \n",
        "  train_accs = [] \n",
        "  \n",
        "  model.train() \n",
        "  for inputs, targets in train_dataloader: \n",
        "    optimizer.zero_grad() \n",
        "\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "    pred = model(inputs)\n",
        "\n",
        "    pred = pred.reshape(-1, pred.size(-1))\n",
        "    targets = targets.reshape(-1)\n",
        "    \n",
        "    loss = criterion(pred, targets) \n",
        "\n",
        "    train_losses.append(loss.item())\n",
        "    train_accs.append(categorical_accuracy(pred, targets)) \n",
        "\n",
        "    loss.backward() \n",
        "    optimizer.step() \n",
        "\n",
        "  model.eval() \n",
        "  y_pred = [] \n",
        "  for inputs in test_dataloader: \n",
        "    inputs = inputs.to(device)\n",
        "    with torch.no_grad(): \n",
        "      pred = model(inputs)\n",
        "      y_pred.append(pred.cpu().detach())\n",
        "    \n",
        "  y_pred = torch.cat(y_pred).numpy() \n",
        "  y_pred = [outputToLabel(y_pred[i],len(y_test[i])) for i in range(len(y_pred))] \n",
        "  test_acc = evaluation_report(y_test, y_pred, get_only_acc=True)  \n",
        "\n",
        "  print(f'epoch = {epoch:02d},\\\n",
        "    training loss = {sum(train_losses) / len(train_losses):.3f}, \\\n",
        "    training acc = {sum(train_accs) / len(train_accs):.3f}, \\\n",
        "    testing acc = {test_acc:.3f}')  "
      ],
      "metadata": {
        "id": "QQaby93eoJYO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fcbe733-168c-4671-9fdb-4a4382fbd6e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=================================================================\n",
            "Layer (type:depth-idx)                   Param #\n",
            "=================================================================\n",
            "BiGRU                                    --\n",
            "├─Embedding: 1-1                         480,608\n",
            "├─GRU: 1-2                               12,672\n",
            "├─Dropout: 1-3                           --\n",
            "├─Linear: 1-4                            3,120\n",
            "=================================================================\n",
            "Total params: 496,400\n",
            "Trainable params: 496,400\n",
            "Non-trainable params: 0\n",
            "=================================================================\n",
            "epoch = 01,    training loss = 1.788,     training acc = 0.572,     testing acc = 0.750\n",
            "epoch = 02,    training loss = 0.803,     training acc = 0.801,     testing acc = 0.826\n",
            "epoch = 03,    training loss = 0.576,     training acc = 0.855,     testing acc = 0.852\n",
            "epoch = 04,    training loss = 0.467,     training acc = 0.879,     testing acc = 0.869\n",
            "epoch = 05,    training loss = 0.397,     training acc = 0.896,     testing acc = 0.881\n",
            "epoch = 06,    training loss = 0.350,     training acc = 0.907,     testing acc = 0.890\n",
            "epoch = 07,    training loss = 0.317,     training acc = 0.915,     testing acc = 0.895\n",
            "epoch = 08,    training loss = 0.292,     training acc = 0.922,     testing acc = 0.899\n",
            "epoch = 09,    training loss = 0.270,     training acc = 0.927,     testing acc = 0.903\n",
            "epoch = 10,    training loss = 0.253,     training acc = 0.932,     testing acc = 0.905\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = [] \n",
        "\n",
        "for inputs in test_dataloader: \n",
        "  inputs = inputs.to(device)\n",
        "  with torch.no_grad(): \n",
        "    pred = model(inputs)\n",
        "    y_pred.append(pred.cpu().detach())\n",
        "  \n",
        "y_pred = torch.cat(y_pred).numpy() \n",
        "y_pred = [outputToLabel(y_pred[i],len(y_test[i])) for i in range(len(y_pred))] \n",
        "evaluation_report(y_test, y_pred) \n"
      ],
      "metadata": {
        "id": "5b5yzXQov1hX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e9d1982b-f8a5-4073-e8d8-efae299aea58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "               tag  precision     recall    f_score correct_count\n",
              "0                1  99.727965  99.484396  99.606032          3666\n",
              "1                2  93.503169  93.004365    93.2531          7671\n",
              "2                3  91.090611  88.868494  89.965833         15009\n",
              "3                4  99.859889  99.280297   99.56925         12829\n",
              "4                5  95.238095  89.552239  92.307692            60\n",
              "5                6  96.969697  85.823755  91.056911           448\n",
              "6                7  98.007775  97.017797  97.510273          2017\n",
              "7                8  24.618736  54.457831  33.908477           226\n",
              "8                9  55.232558  51.630435  53.370787           190\n",
              "9               10  51.211073  35.280095  41.778405           296\n",
              "10              11       96.0   83.72093  89.440994            72\n",
              "11              12  94.927536  98.127341  96.500921           786\n",
              "12              13  85.426573  84.974958  85.200167          3054\n",
              "13              14  94.417833  93.368555  93.890263          5125\n",
              "14              15  76.044852  66.488414  70.946267           746\n",
              "15              16  87.027027   87.13572   87.08134          2093\n",
              "16              17  96.640316  83.732877  89.724771           489\n",
              "17              18   96.77693  98.787879  97.772065          1141\n",
              "18              19  92.774566  93.043478  92.908828           321\n",
              "19              20   99.29078  94.915254  97.053726           280\n",
              "20              21   92.43807   93.59736  93.014103          1418\n",
              "21              22  81.187411  70.804741  75.641453          1135\n",
              "22              23  88.451444  95.535082  91.856899          1348\n",
              "23              24  45.048104  86.994536  59.358688           796\n",
              "24              25  76.608187  63.438257  69.403974           262\n",
              "25              26   91.27907  89.204545  90.229885           157\n",
              "26              27  92.035398  79.389313  85.245902           104\n",
              "27              29  95.873016   95.56962  95.721078           302\n",
              "28              30  81.111111  71.568627  76.041667            73\n",
              "29              31  49.710983  83.495146  62.318841            86\n",
              "30              32  68.571429  53.932584  60.377358            96\n",
              "31              33          -        0.0          -             0\n",
              "32              34  84.627575  95.017794  89.522213           534\n",
              "33              35      100.0  44.444444  61.538462             4\n",
              "34              36      100.0      100.0      100.0            16\n",
              "35              37  94.392523  99.019608  96.650718           101\n",
              "36              38       50.0  17.948718  26.415094             7\n",
              "37              39   76.07362  88.571429  81.848185           124\n",
              "38              40  97.192982  98.928571  98.053097           277\n",
              "39              41  85.714286       30.0  44.444444             6\n",
              "40              42      100.0  64.705882  78.571429            11\n",
              "41              43          -        0.0          -             0\n",
              "42              45          -        0.0          -             0\n",
              "43              46          -        0.0          -             0\n",
              "44  accuracy=90.49                                               "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c1167ac7-d2d1-4dbb-a630-86b6262d38c4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tag</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f_score</th>\n",
              "      <th>correct_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>99.727965</td>\n",
              "      <td>99.484396</td>\n",
              "      <td>99.606032</td>\n",
              "      <td>3666</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>93.503169</td>\n",
              "      <td>93.004365</td>\n",
              "      <td>93.2531</td>\n",
              "      <td>7671</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>91.090611</td>\n",
              "      <td>88.868494</td>\n",
              "      <td>89.965833</td>\n",
              "      <td>15009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>99.859889</td>\n",
              "      <td>99.280297</td>\n",
              "      <td>99.56925</td>\n",
              "      <td>12829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>95.238095</td>\n",
              "      <td>89.552239</td>\n",
              "      <td>92.307692</td>\n",
              "      <td>60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>96.969697</td>\n",
              "      <td>85.823755</td>\n",
              "      <td>91.056911</td>\n",
              "      <td>448</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>98.007775</td>\n",
              "      <td>97.017797</td>\n",
              "      <td>97.510273</td>\n",
              "      <td>2017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>24.618736</td>\n",
              "      <td>54.457831</td>\n",
              "      <td>33.908477</td>\n",
              "      <td>226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>55.232558</td>\n",
              "      <td>51.630435</td>\n",
              "      <td>53.370787</td>\n",
              "      <td>190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>51.211073</td>\n",
              "      <td>35.280095</td>\n",
              "      <td>41.778405</td>\n",
              "      <td>296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>11</td>\n",
              "      <td>96.0</td>\n",
              "      <td>83.72093</td>\n",
              "      <td>89.440994</td>\n",
              "      <td>72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>12</td>\n",
              "      <td>94.927536</td>\n",
              "      <td>98.127341</td>\n",
              "      <td>96.500921</td>\n",
              "      <td>786</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>13</td>\n",
              "      <td>85.426573</td>\n",
              "      <td>84.974958</td>\n",
              "      <td>85.200167</td>\n",
              "      <td>3054</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>14</td>\n",
              "      <td>94.417833</td>\n",
              "      <td>93.368555</td>\n",
              "      <td>93.890263</td>\n",
              "      <td>5125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>15</td>\n",
              "      <td>76.044852</td>\n",
              "      <td>66.488414</td>\n",
              "      <td>70.946267</td>\n",
              "      <td>746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>16</td>\n",
              "      <td>87.027027</td>\n",
              "      <td>87.13572</td>\n",
              "      <td>87.08134</td>\n",
              "      <td>2093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>17</td>\n",
              "      <td>96.640316</td>\n",
              "      <td>83.732877</td>\n",
              "      <td>89.724771</td>\n",
              "      <td>489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>18</td>\n",
              "      <td>96.77693</td>\n",
              "      <td>98.787879</td>\n",
              "      <td>97.772065</td>\n",
              "      <td>1141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>19</td>\n",
              "      <td>92.774566</td>\n",
              "      <td>93.043478</td>\n",
              "      <td>92.908828</td>\n",
              "      <td>321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>20</td>\n",
              "      <td>99.29078</td>\n",
              "      <td>94.915254</td>\n",
              "      <td>97.053726</td>\n",
              "      <td>280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>21</td>\n",
              "      <td>92.43807</td>\n",
              "      <td>93.59736</td>\n",
              "      <td>93.014103</td>\n",
              "      <td>1418</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>22</td>\n",
              "      <td>81.187411</td>\n",
              "      <td>70.804741</td>\n",
              "      <td>75.641453</td>\n",
              "      <td>1135</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>23</td>\n",
              "      <td>88.451444</td>\n",
              "      <td>95.535082</td>\n",
              "      <td>91.856899</td>\n",
              "      <td>1348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>24</td>\n",
              "      <td>45.048104</td>\n",
              "      <td>86.994536</td>\n",
              "      <td>59.358688</td>\n",
              "      <td>796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>25</td>\n",
              "      <td>76.608187</td>\n",
              "      <td>63.438257</td>\n",
              "      <td>69.403974</td>\n",
              "      <td>262</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>26</td>\n",
              "      <td>91.27907</td>\n",
              "      <td>89.204545</td>\n",
              "      <td>90.229885</td>\n",
              "      <td>157</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>27</td>\n",
              "      <td>92.035398</td>\n",
              "      <td>79.389313</td>\n",
              "      <td>85.245902</td>\n",
              "      <td>104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>29</td>\n",
              "      <td>95.873016</td>\n",
              "      <td>95.56962</td>\n",
              "      <td>95.721078</td>\n",
              "      <td>302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>30</td>\n",
              "      <td>81.111111</td>\n",
              "      <td>71.568627</td>\n",
              "      <td>76.041667</td>\n",
              "      <td>73</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>31</td>\n",
              "      <td>49.710983</td>\n",
              "      <td>83.495146</td>\n",
              "      <td>62.318841</td>\n",
              "      <td>86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>32</td>\n",
              "      <td>68.571429</td>\n",
              "      <td>53.932584</td>\n",
              "      <td>60.377358</td>\n",
              "      <td>96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>33</td>\n",
              "      <td>-</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>34</td>\n",
              "      <td>84.627575</td>\n",
              "      <td>95.017794</td>\n",
              "      <td>89.522213</td>\n",
              "      <td>534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>35</td>\n",
              "      <td>100.0</td>\n",
              "      <td>44.444444</td>\n",
              "      <td>61.538462</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>36</td>\n",
              "      <td>100.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>37</td>\n",
              "      <td>94.392523</td>\n",
              "      <td>99.019608</td>\n",
              "      <td>96.650718</td>\n",
              "      <td>101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>38</td>\n",
              "      <td>50.0</td>\n",
              "      <td>17.948718</td>\n",
              "      <td>26.415094</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>39</td>\n",
              "      <td>76.07362</td>\n",
              "      <td>88.571429</td>\n",
              "      <td>81.848185</td>\n",
              "      <td>124</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>40</td>\n",
              "      <td>97.192982</td>\n",
              "      <td>98.928571</td>\n",
              "      <td>98.053097</td>\n",
              "      <td>277</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>41</td>\n",
              "      <td>85.714286</td>\n",
              "      <td>30.0</td>\n",
              "      <td>44.444444</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>42</td>\n",
              "      <td>100.0</td>\n",
              "      <td>64.705882</td>\n",
              "      <td>78.571429</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>43</td>\n",
              "      <td>-</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>45</td>\n",
              "      <td>-</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>46</td>\n",
              "      <td>-</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>accuracy=90.49</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c1167ac7-d2d1-4dbb-a630-86b6262d38c4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c1167ac7-d2d1-4dbb-a630-86b6262d38c4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c1167ac7-d2d1-4dbb-a630-86b6262d38c4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0NKka14hHQw"
      },
      "source": [
        "## 4.2 CRF Viterbi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijd1rwTghHQx"
      },
      "source": [
        "Your next task is to incorporate Conditional random fields (CRF) to your model. <b>You do not need to use pretrained weight</b>.\n",
        "\n",
        "To use the CRF layer, you need to use an extension repository for pytorch library, call torch2crf. If you want to see the detailed implementation, you should read the official pytorch tutorial of CRF (https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html). \n",
        "\n",
        "torch2crf link :  https://github.com/kmkurn/pytorch-crf\n",
        "\n",
        "For inference, you should look at crf.py at the method call and view the input/output argmunets. \n",
        "\n",
        "link for documentation: https://pytorch-crf.readthedocs.io/en/stable/\n",
        "\n",
        "link for source code : https://github.com/kmkurn/pytorch-crf/blob/master/torchcrf/__init__.py\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2.1 CRF without pretrained weight\n",
        "### #TODO 1\n",
        "Incoperate CRF layer to your model in 4.1. CRF is quite complex compare to previous example model, so you should train it with more epoch, so it can converge.\n",
        "\n",
        "To finish this excercise you must train the model and show the evaluation report with this model as shown in the example.\n",
        "\n",
        "Do not forget to save this model weight. (Refer to https://pytorch.org/tutorials/beginner/saving_loading_models.html)"
      ],
      "metadata": {
        "id": "YLfjTd2d7oP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# INSERT YOUR CODE HERE\n",
        "\n",
        "import torchinfo \n",
        "from torch import nn \n",
        "from torch.nn import Embedding, Dropout, GRU, LSTM, Linear, CrossEntropyLoss \n",
        "from torch.optim import Adam\n",
        "from torchcrf import CRF\n",
        "\n",
        "class BiGRU_CRF(nn.Module): \n",
        "  def __init__(self, word_to_idx):\n",
        "    super(BiGRU_CRF, self).__init__() \n",
        "    self.embed = Embedding(len(word_to_idx), 32, padding_idx=0)\n",
        "    self.bi_gru = GRU(32, 32, bidirectional=True, batch_first=True) \n",
        "    self.classifier = Linear(64, 48)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = self.embed(x) \n",
        "    x, _ = self.bi_gru(x)\n",
        "    out = self.classifier(x)\n",
        "    return out"
      ],
      "metadata": {
        "id": "iBb8h8s1_IBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model_BiGRU_CRF = BiGRU_CRF(word_to_idx).to(device)\n",
        "\n",
        "optimizer = Adam(model_BiGRU_CRF.parameters(), lr=1e-3)\n",
        "criterion_CRF = CRF(48, batch_first=True).to(device)\n",
        "print(torchinfo.summary(model_BiGRU_CRF))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWytWydRNop6",
        "outputId": "bbf42304-f6a9-4ecf-8653-0310c10dfa55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=================================================================\n",
            "Layer (type:depth-idx)                   Param #\n",
            "=================================================================\n",
            "BiGRU_CRF                                --\n",
            "├─Embedding: 1-1                         480,608\n",
            "├─GRU: 1-2                               12,672\n",
            "├─Linear: 1-3                            3,120\n",
            "=================================================================\n",
            "Total params: 496,400\n",
            "Trainable params: 496,400\n",
            "Non-trainable params: 0\n",
            "=================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(1, num_epochs+1): \n",
        "  train_losses = [] \n",
        "  train_accs = [] \n",
        "  \n",
        "  model_BiGRU_CRF.train() \n",
        "  for inputs, targets in train_dataloader: \n",
        "    optimizer.zero_grad() \n",
        "\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "    pred = model_BiGRU_CRF(inputs)\n",
        "\n",
        "    loss = -criterion_CRF(pred, targets)\n",
        "\n",
        "    pred = pred.reshape(-1, pred.size(-1))\n",
        "    targets = targets.reshape(-1)\n",
        "\n",
        "    train_losses.append(loss.item())\n",
        "    train_accs.append(categorical_accuracy(pred, targets)) \n",
        "\n",
        "    loss.backward() \n",
        "    optimizer.step() \n",
        "\n",
        "  model_BiGRU_CRF.eval() \n",
        "  y_pred = [] \n",
        "  for inputs in test_dataloader: \n",
        "    inputs = inputs.to(device)\n",
        "    with torch.no_grad(): \n",
        "      pred = model_BiGRU_CRF(inputs)\n",
        "      y_pred.append(pred.cpu().detach())\n",
        "    \n",
        "  y_pred = torch.cat(y_pred).numpy() \n",
        "  y_pred = [outputToLabel(y_pred[i],len(y_test[i])) for i in range(len(y_pred))] \n",
        "  test_acc = evaluation_report(y_test, y_pred, get_only_acc=True)  \n",
        "\n",
        "  print(f'epoch = {epoch:02d},\\\n",
        "    training loss = {sum(train_losses) / len(train_losses):.3f}, \\\n",
        "    training acc = {sum(train_accs) / len(train_accs):.3f}, \\\n",
        "    testing acc = {test_acc:.3f}')  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShBcQzs0Nraa",
        "outputId": "0ef008ae-d819-4f68-8b5f-82f0b9f31c49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch = 01,    training loss = 4881.409,     training acc = 0.445,     testing acc = 0.656\n",
            "epoch = 02,    training loss = 1008.821,     training acc = 0.748,     testing acc = 0.791\n",
            "epoch = 03,    training loss = 650.311,     training acc = 0.834,     testing acc = 0.843\n",
            "epoch = 04,    training loss = 488.887,     training acc = 0.874,     testing acc = 0.868\n",
            "epoch = 05,    training loss = 396.659,     training acc = 0.896,     testing acc = 0.884\n",
            "epoch = 06,    training loss = 336.953,     training acc = 0.909,     testing acc = 0.897\n",
            "epoch = 07,    training loss = 295.740,     training acc = 0.919,     testing acc = 0.904\n",
            "epoch = 08,    training loss = 264.625,     training acc = 0.926,     testing acc = 0.909\n",
            "epoch = 09,    training loss = 240.027,     training acc = 0.933,     testing acc = 0.913\n",
            "epoch = 10,    training loss = 219.973,     training acc = 0.937,     testing acc = 0.916\n",
            "epoch = 11,    training loss = 203.318,     training acc = 0.942,     testing acc = 0.919\n",
            "epoch = 12,    training loss = 189.260,     training acc = 0.945,     testing acc = 0.921\n",
            "epoch = 13,    training loss = 177.145,     training acc = 0.949,     testing acc = 0.922\n",
            "epoch = 14,    training loss = 166.817,     training acc = 0.951,     testing acc = 0.924\n",
            "epoch = 15,    training loss = 157.606,     training acc = 0.953,     testing acc = 0.925\n",
            "epoch = 16,    training loss = 149.351,     training acc = 0.955,     testing acc = 0.927\n",
            "epoch = 17,    training loss = 142.148,     training acc = 0.957,     testing acc = 0.928\n",
            "epoch = 18,    training loss = 135.554,     training acc = 0.959,     testing acc = 0.928\n",
            "epoch = 19,    training loss = 129.643,     training acc = 0.961,     testing acc = 0.929\n",
            "epoch = 20,    training loss = 124.299,     training acc = 0.962,     testing acc = 0.928\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model_BiGRU_CRF.state_dict(), \"model_BiGRU_CRF.pt\")"
      ],
      "metadata": {
        "id": "u658AjQXlN7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_BiGRU_CRF = BiGRU_CRF(word_to_idx).to(device)\n",
        "model_BiGRU_CRF.load_state_dict(torch.load(\"model_BiGRU_CRF.pt\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1PXNTmt6ldAd",
        "outputId": "f2c04e55-5979-4fef-e050-5a39910a5a42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = [] \n",
        "\n",
        "for inputs in test_dataloader: \n",
        "  inputs = inputs.to(device)\n",
        "  with torch.no_grad(): \n",
        "    pred = model_BiGRU_CRF(inputs)\n",
        "    y_pred.append(pred.cpu().detach())\n",
        "  \n",
        "y_pred = torch.cat(y_pred).numpy() \n",
        "y_pred = [outputToLabel(y_pred[i],len(y_test[i])) for i in range(len(y_pred))] \n",
        "evaluation_report(y_test, y_pred) \n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OYIv_UpCkqSV",
        "outputId": "661d8c1f-04ef-4deb-a42e-bc6e97cf989b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "               tag  precision     recall    f_score correct_count\n",
              "0                0        0.0          0          -             0\n",
              "1                1  99.891097  99.565807  99.728187          3669\n",
              "2                2  94.803922  93.792435  94.295466          7736\n",
              "3                3  90.911647  95.713186  93.250649         16165\n",
              "4                4  99.891304  99.566631  99.728703         12866\n",
              "5                5  95.652174  98.507463  97.058824            66\n",
              "6                6  97.198276  86.398467   91.48073           451\n",
              "7                7  96.380952  97.354497  96.865279          2024\n",
              "8                8  65.449438  56.144578  60.440986           233\n",
              "9                9  67.801858   59.51087  63.386397           219\n",
              "10              10  55.144695  40.882002  46.954141           343\n",
              "11              11  82.828283  95.348837  88.648649            82\n",
              "12              12  95.940959  97.378277  96.654275           780\n",
              "13              13  86.449255  85.559265  86.001958          3075\n",
              "14              14  94.781176  94.297686  94.538813          5176\n",
              "15              15  83.143744  71.657754  76.974629           804\n",
              "16              16  87.682064  87.718568  87.700312          2107\n",
              "17              17  97.768763  82.534247  89.507892           482\n",
              "18              18  97.451147  99.307359  98.370497          1147\n",
              "19              19  98.203593  95.072464  96.612666           328\n",
              "20              20  98.961938  96.949153  97.945205           286\n",
              "21              21    94.9419  91.683168  93.284083          1389\n",
              "22              22   83.70565  79.475983     81.536          1274\n",
              "23              23  89.579158  95.038979  92.228336          1341\n",
              "24              24  89.655172  82.404372  85.876993           754\n",
              "25              25   87.42515  70.702179  78.179384           292\n",
              "26              26  93.063584  91.477273   92.26361           161\n",
              "27              27  92.173913  80.916031  86.178862           106\n",
              "28              29   93.76947  95.253165  94.505495           301\n",
              "29              30  74.257426  73.529412  73.891626            75\n",
              "30              31  46.938776  89.320388  61.538462            92\n",
              "31              32  73.758865  58.426966  65.203762           104\n",
              "32              33  77.419355  35.294118  48.484848            24\n",
              "33              34  90.750436   92.52669  91.629956           520\n",
              "34              35  83.333333  55.555556  66.666667             5\n",
              "35              36      100.0      81.25  89.655172            13\n",
              "36              37  91.818182  99.019608  95.283019           101\n",
              "37              38      56.25  46.153846  50.704225            18\n",
              "38              39  75.912409  74.285714  75.090253           104\n",
              "39              40  98.586572  99.642857  99.111901           279\n",
              "40              41       87.5       35.0       50.0             7\n",
              "41              42      100.0  64.705882  78.571429            11\n",
              "42              43       75.0  33.333333  46.153846             3\n",
              "43              45          -        0.0          -             0\n",
              "44              46          -        0.0          -             0\n",
              "45  accuracy=92.83                                               "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-153b40b3-73e6-4c22-b494-5b0a0d1c2ad1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tag</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f_score</th>\n",
              "      <th>correct_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>99.891097</td>\n",
              "      <td>99.565807</td>\n",
              "      <td>99.728187</td>\n",
              "      <td>3669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>94.803922</td>\n",
              "      <td>93.792435</td>\n",
              "      <td>94.295466</td>\n",
              "      <td>7736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>90.911647</td>\n",
              "      <td>95.713186</td>\n",
              "      <td>93.250649</td>\n",
              "      <td>16165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>99.891304</td>\n",
              "      <td>99.566631</td>\n",
              "      <td>99.728703</td>\n",
              "      <td>12866</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>95.652174</td>\n",
              "      <td>98.507463</td>\n",
              "      <td>97.058824</td>\n",
              "      <td>66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>97.198276</td>\n",
              "      <td>86.398467</td>\n",
              "      <td>91.48073</td>\n",
              "      <td>451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>96.380952</td>\n",
              "      <td>97.354497</td>\n",
              "      <td>96.865279</td>\n",
              "      <td>2024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>65.449438</td>\n",
              "      <td>56.144578</td>\n",
              "      <td>60.440986</td>\n",
              "      <td>233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>67.801858</td>\n",
              "      <td>59.51087</td>\n",
              "      <td>63.386397</td>\n",
              "      <td>219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>55.144695</td>\n",
              "      <td>40.882002</td>\n",
              "      <td>46.954141</td>\n",
              "      <td>343</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>82.828283</td>\n",
              "      <td>95.348837</td>\n",
              "      <td>88.648649</td>\n",
              "      <td>82</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>95.940959</td>\n",
              "      <td>97.378277</td>\n",
              "      <td>96.654275</td>\n",
              "      <td>780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>86.449255</td>\n",
              "      <td>85.559265</td>\n",
              "      <td>86.001958</td>\n",
              "      <td>3075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>14</td>\n",
              "      <td>94.781176</td>\n",
              "      <td>94.297686</td>\n",
              "      <td>94.538813</td>\n",
              "      <td>5176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>15</td>\n",
              "      <td>83.143744</td>\n",
              "      <td>71.657754</td>\n",
              "      <td>76.974629</td>\n",
              "      <td>804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>16</td>\n",
              "      <td>87.682064</td>\n",
              "      <td>87.718568</td>\n",
              "      <td>87.700312</td>\n",
              "      <td>2107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>17</td>\n",
              "      <td>97.768763</td>\n",
              "      <td>82.534247</td>\n",
              "      <td>89.507892</td>\n",
              "      <td>482</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>18</td>\n",
              "      <td>97.451147</td>\n",
              "      <td>99.307359</td>\n",
              "      <td>98.370497</td>\n",
              "      <td>1147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>19</td>\n",
              "      <td>98.203593</td>\n",
              "      <td>95.072464</td>\n",
              "      <td>96.612666</td>\n",
              "      <td>328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>20</td>\n",
              "      <td>98.961938</td>\n",
              "      <td>96.949153</td>\n",
              "      <td>97.945205</td>\n",
              "      <td>286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>21</td>\n",
              "      <td>94.9419</td>\n",
              "      <td>91.683168</td>\n",
              "      <td>93.284083</td>\n",
              "      <td>1389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>22</td>\n",
              "      <td>83.70565</td>\n",
              "      <td>79.475983</td>\n",
              "      <td>81.536</td>\n",
              "      <td>1274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>23</td>\n",
              "      <td>89.579158</td>\n",
              "      <td>95.038979</td>\n",
              "      <td>92.228336</td>\n",
              "      <td>1341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>24</td>\n",
              "      <td>89.655172</td>\n",
              "      <td>82.404372</td>\n",
              "      <td>85.876993</td>\n",
              "      <td>754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>25</td>\n",
              "      <td>87.42515</td>\n",
              "      <td>70.702179</td>\n",
              "      <td>78.179384</td>\n",
              "      <td>292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>26</td>\n",
              "      <td>93.063584</td>\n",
              "      <td>91.477273</td>\n",
              "      <td>92.26361</td>\n",
              "      <td>161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>27</td>\n",
              "      <td>92.173913</td>\n",
              "      <td>80.916031</td>\n",
              "      <td>86.178862</td>\n",
              "      <td>106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>29</td>\n",
              "      <td>93.76947</td>\n",
              "      <td>95.253165</td>\n",
              "      <td>94.505495</td>\n",
              "      <td>301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>30</td>\n",
              "      <td>74.257426</td>\n",
              "      <td>73.529412</td>\n",
              "      <td>73.891626</td>\n",
              "      <td>75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>31</td>\n",
              "      <td>46.938776</td>\n",
              "      <td>89.320388</td>\n",
              "      <td>61.538462</td>\n",
              "      <td>92</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>32</td>\n",
              "      <td>73.758865</td>\n",
              "      <td>58.426966</td>\n",
              "      <td>65.203762</td>\n",
              "      <td>104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>33</td>\n",
              "      <td>77.419355</td>\n",
              "      <td>35.294118</td>\n",
              "      <td>48.484848</td>\n",
              "      <td>24</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>34</td>\n",
              "      <td>90.750436</td>\n",
              "      <td>92.52669</td>\n",
              "      <td>91.629956</td>\n",
              "      <td>520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>35</td>\n",
              "      <td>83.333333</td>\n",
              "      <td>55.555556</td>\n",
              "      <td>66.666667</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>36</td>\n",
              "      <td>100.0</td>\n",
              "      <td>81.25</td>\n",
              "      <td>89.655172</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>37</td>\n",
              "      <td>91.818182</td>\n",
              "      <td>99.019608</td>\n",
              "      <td>95.283019</td>\n",
              "      <td>101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>38</td>\n",
              "      <td>56.25</td>\n",
              "      <td>46.153846</td>\n",
              "      <td>50.704225</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>39</td>\n",
              "      <td>75.912409</td>\n",
              "      <td>74.285714</td>\n",
              "      <td>75.090253</td>\n",
              "      <td>104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>40</td>\n",
              "      <td>98.586572</td>\n",
              "      <td>99.642857</td>\n",
              "      <td>99.111901</td>\n",
              "      <td>279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>41</td>\n",
              "      <td>87.5</td>\n",
              "      <td>35.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>42</td>\n",
              "      <td>100.0</td>\n",
              "      <td>64.705882</td>\n",
              "      <td>78.571429</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>43</td>\n",
              "      <td>75.0</td>\n",
              "      <td>33.333333</td>\n",
              "      <td>46.153846</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>45</td>\n",
              "      <td>-</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>46</td>\n",
              "      <td>-</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>accuracy=92.83</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-153b40b3-73e6-4c22-b494-5b0a0d1c2ad1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-153b40b3-73e6-4c22-b494-5b0a0d1c2ad1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-153b40b3-73e6-4c22-b494-5b0a0d1c2ad1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuybajePhHQy"
      },
      "source": [
        "### #TODO 2\n",
        "We would like you create a neural CRF postagger model  with the pretrained word embedding as an input and the word embedding is trainable (not fixed). To finish this excercise you must train the model and show the evaluation report with this model as shown in the example.\n",
        "\n",
        "Please note that the given pretrained word embedding only have weights for the vocabuary in BEST corpus.\n",
        "\n",
        "Optionally, you can use your own pretrained word embedding.\n",
        "\n",
        "<B> Hint: You can get the embedding from get_embeddings function from embeddings/emb_reader.py . </b>\n",
        "\n",
        "(You may want to read about Trainable parameter)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from embeddings import emb_reader # load emb_reader.py from PATH\n",
        "pretrained_embeddings = emb_reader.get_embeddings()"
      ],
      "metadata": {
        "id": "VEW0plVUnyL4"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_embeddings_weight = [pretrained_embeddings['PAD']] + [ pretrained_embeddings[idx_to_word[idx]]\n",
        "                                if idx_to_word[idx] in pretrained_embeddings\n",
        "                                else pretrained_embeddings['<UNK>']\n",
        "                                for idx in idx_to_word\n",
        "                                ]\n",
        "pretrained_embeddings_weight = torch.Tensor(pretrained_embeddings_weight)\n",
        "print(pretrained_embeddings_weight.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0m4qXMOXVFt",
        "outputId": "edcf3d16-a7d8-432d-af1f-e18c6175358c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([15019, 64])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-d7f9c9417c81>:6: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
            "  pretrained_embeddings_weight = torch.Tensor(pretrained_embeddings_weight)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# INSERT YOUR CODE HERE\n",
        "\n",
        "import torchinfo \n",
        "from torch import nn \n",
        "from torch.nn import Embedding, Dropout, GRU, LSTM, Linear, CrossEntropyLoss \n",
        "from torch.optim import Adam\n",
        "from torchcrf import CRF\n",
        "\n",
        "class BiGRU_CRF_pretrained(nn.Module): \n",
        "  def __init__(self, word_to_idx):\n",
        "    super(BiGRU_CRF_pretrained, self).__init__() \n",
        "    self.embed = Embedding(len(word_to_idx), 64, padding_idx=0, _weight=pretrained_embeddings_weight)\n",
        "    self.bi_gru = GRU(64, 32, bidirectional=True, batch_first=True) \n",
        "    self.classifier = Linear(64, 48)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = self.embed(x) \n",
        "    x, _ = self.bi_gru(x)\n",
        "    out = self.classifier(x)\n",
        "    return out"
      ],
      "metadata": {
        "id": "NhH4-njp_LQU"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "model_BiGRU_CRF_pretrained = BiGRU_CRF_pretrained(word_to_idx).to(device)\n",
        "\n",
        "optimizer = Adam(model_BiGRU_CRF_pretrained.parameters(), lr=1e-3)\n",
        "criterion_CRF_pretrained = CRF(48, batch_first=True).to(device)\n",
        "print(torchinfo.summary(model_BiGRU_CRF_pretrained))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HS55JlOyk-t-",
        "outputId": "25f14080-4625-4afd-bcc0-50dedd9d89a1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=================================================================\n",
            "Layer (type:depth-idx)                   Param #\n",
            "=================================================================\n",
            "BiGRU_CRF_pretrained                     --\n",
            "├─Embedding: 1-1                         961,216\n",
            "├─GRU: 1-2                               18,816\n",
            "├─Linear: 1-3                            3,120\n",
            "=================================================================\n",
            "Total params: 983,152\n",
            "Trainable params: 983,152\n",
            "Non-trainable params: 0\n",
            "=================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 10 \n",
        "\n",
        "for epoch in range(1, num_epochs+1): \n",
        "  train_losses = [] \n",
        "  train_accs = [] \n",
        "  \n",
        "  model_BiGRU_CRF_pretrained.train() \n",
        "  for inputs, targets in train_dataloader: \n",
        "    optimizer.zero_grad() \n",
        "\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "    pred = model_BiGRU_CRF_pretrained(inputs)\n",
        "\n",
        "    loss = -criterion_CRF_pretrained(pred, targets)\n",
        "\n",
        "    pred = pred.reshape(-1, pred.size(-1))\n",
        "    targets = targets.reshape(-1)\n",
        "\n",
        "    train_losses.append(loss.item())\n",
        "    train_accs.append(categorical_accuracy(pred, targets)) \n",
        "\n",
        "    loss.backward() \n",
        "    optimizer.step() \n",
        "\n",
        "  model_BiGRU_CRF_pretrained.eval() \n",
        "  y_pred = [] \n",
        "  for inputs in test_dataloader: \n",
        "    inputs = inputs.to(device)\n",
        "    with torch.no_grad(): \n",
        "      pred = model_BiGRU_CRF_pretrained(inputs)\n",
        "      y_pred.append(pred.cpu().detach())\n",
        "    \n",
        "  y_pred = torch.cat(y_pred).numpy() \n",
        "  y_pred = [outputToLabel(y_pred[i],len(y_test[i])) for i in range(len(y_pred))] \n",
        "  test_acc = evaluation_report(y_test, y_pred, get_only_acc=True)  \n",
        "\n",
        "  print(f'epoch = {epoch:02d},\\\n",
        "    training loss = {sum(train_losses) / len(train_losses):.3f}, \\\n",
        "    training acc = {sum(train_accs) / len(train_accs):.3f}, \\\n",
        "    testing acc = {test_acc:.3f}')  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "689kh1_0lCh-",
        "outputId": "75568a4f-0086-4b71-df3f-8f9858a30acb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch = 01,    training loss = 4562.286,     training acc = 0.318,     testing acc = 0.672\n",
            "epoch = 02,    training loss = 884.104,     training acc = 0.800,     testing acc = 0.857\n",
            "epoch = 03,    training loss = 443.194,     training acc = 0.895,     testing acc = 0.897\n",
            "epoch = 04,    training loss = 288.577,     training acc = 0.927,     testing acc = 0.916\n",
            "epoch = 05,    training loss = 218.866,     training acc = 0.942,     testing acc = 0.922\n",
            "epoch = 06,    training loss = 182.505,     training acc = 0.949,     testing acc = 0.925\n",
            "epoch = 07,    training loss = 160.808,     training acc = 0.953,     testing acc = 0.927\n",
            "epoch = 08,    training loss = 145.958,     training acc = 0.956,     testing acc = 0.928\n",
            "epoch = 09,    training loss = 134.480,     training acc = 0.959,     testing acc = 0.931\n",
            "epoch = 10,    training loss = 125.285,     training acc = 0.961,     testing acc = 0.931\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model_BiGRU_CRF_pretrained.state_dict(), \"model_BiGRU_CRF_pretrained.pt\")"
      ],
      "metadata": {
        "id": "CpEJS3QorXZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_BiGRU_CRF_pretrained = BiGRU_CRF_pretrained(word_to_idx).to(device)\n",
        "model_BiGRU_CRF_pretrained.load_state_dict(torch.load(\"model_BiGRU_CRF_pretrained.pt\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJ4_xWtUrc65",
        "outputId": "2ae9ff01-67ce-4dce-87c6-60797e0fdd2d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = [] \n",
        "\n",
        "for inputs in test_dataloader: \n",
        "  inputs = inputs.to(device)\n",
        "  with torch.no_grad(): \n",
        "    pred = model_BiGRU_CRF_pretrained(inputs)\n",
        "    y_pred.append(pred.cpu().detach())\n",
        "  \n",
        "y_pred = torch.cat(y_pred).numpy() \n",
        "y_pred = [outputToLabel(y_pred[i],len(y_test[i])) for i in range(len(y_pred))] \n",
        "evaluation_report(y_test, y_pred) \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fHEHxYOarmVW",
        "outputId": "05160822-312b-4e39-a17c-7949c265c1cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "               tag  precision     recall    f_score correct_count\n",
              "0                0        0.0          0          -             0\n",
              "1                1  99.891127  99.592944  99.741813          3670\n",
              "2                2  94.626426  93.513579  94.066711          7713\n",
              "3                3  90.741679  95.405293  93.015067         16113\n",
              "4                4  99.930151  99.644018   99.78688         12876\n",
              "5                5  95.652174  98.507463  97.058824            66\n",
              "6                6      100.0  87.547893  93.360572           457\n",
              "7                7   97.12506  97.498797   97.31157          2027\n",
              "8                8  73.381295  49.156627  58.874459           204\n",
              "9                9  75.153374  66.576087  70.605187           245\n",
              "10              10  63.829787  42.908224  51.318603           360\n",
              "11              11  86.734694  98.837209  92.391304            85\n",
              "12              12   97.64268  98.252185  97.946484           787\n",
              "13              13  87.954545  86.143573   87.03964          3096\n",
              "14              14  94.605431   94.57096  94.588192          5191\n",
              "15              15  82.474227  71.301248  76.481836           800\n",
              "16              16  88.682363  89.383847  89.031723          2147\n",
              "17              17  97.959184  90.410959  94.033838           528\n",
              "18              18  96.624473  99.134199  97.863248          1145\n",
              "19              19  97.076023  96.231884  96.652111           332\n",
              "20              20  98.281787  96.949153  97.610922           286\n",
              "21              21  94.493542  91.749175  93.101139          1390\n",
              "22              22  84.672021  78.914535   81.69196          1265\n",
              "23              23  89.933775  96.243799  92.981856          1358\n",
              "24              24  88.255034  86.229508  87.230514           789\n",
              "25              25  89.355742  77.239709  82.857143           319\n",
              "26              26  89.502762  92.045455  90.756303           162\n",
              "27              27  90.990991  77.099237  83.471074           101\n",
              "28              29  93.072289   97.78481   95.37037           309\n",
              "29              30  77.777778  75.490196  76.616915            77\n",
              "30              31  67.605634  93.203883  78.367347            96\n",
              "31              32  83.035714  52.247191  64.137931            93\n",
              "32              33  85.714286  44.117647  58.252427            30\n",
              "33              34  90.909091   92.52669  91.710758           520\n",
              "34              35  83.333333  55.555556  66.666667             5\n",
              "35              36      100.0       62.5  76.923077            10\n",
              "36              37  93.518519  99.019608  96.190476           101\n",
              "37              38   69.69697  58.974359  63.888889            23\n",
              "38              39  77.241379       80.0  78.596491           112\n",
              "39              40      100.0  99.642857  99.821109           279\n",
              "40              41       80.0       60.0  68.571429            12\n",
              "41              42      100.0  64.705882  78.571429            11\n",
              "42              43          -        0.0          -             0\n",
              "43              45          -        0.0          -             0\n",
              "44              46          -        0.0          -             0\n",
              "45  accuracy=93.08                                               "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-75605548-9291-4a06-a3d6-43f637b173a7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tag</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f_score</th>\n",
              "      <th>correct_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>99.891127</td>\n",
              "      <td>99.592944</td>\n",
              "      <td>99.741813</td>\n",
              "      <td>3670</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>94.626426</td>\n",
              "      <td>93.513579</td>\n",
              "      <td>94.066711</td>\n",
              "      <td>7713</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>90.741679</td>\n",
              "      <td>95.405293</td>\n",
              "      <td>93.015067</td>\n",
              "      <td>16113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>99.930151</td>\n",
              "      <td>99.644018</td>\n",
              "      <td>99.78688</td>\n",
              "      <td>12876</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>95.652174</td>\n",
              "      <td>98.507463</td>\n",
              "      <td>97.058824</td>\n",
              "      <td>66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "      <td>100.0</td>\n",
              "      <td>87.547893</td>\n",
              "      <td>93.360572</td>\n",
              "      <td>457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>7</td>\n",
              "      <td>97.12506</td>\n",
              "      <td>97.498797</td>\n",
              "      <td>97.31157</td>\n",
              "      <td>2027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>8</td>\n",
              "      <td>73.381295</td>\n",
              "      <td>49.156627</td>\n",
              "      <td>58.874459</td>\n",
              "      <td>204</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>75.153374</td>\n",
              "      <td>66.576087</td>\n",
              "      <td>70.605187</td>\n",
              "      <td>245</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>10</td>\n",
              "      <td>63.829787</td>\n",
              "      <td>42.908224</td>\n",
              "      <td>51.318603</td>\n",
              "      <td>360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>11</td>\n",
              "      <td>86.734694</td>\n",
              "      <td>98.837209</td>\n",
              "      <td>92.391304</td>\n",
              "      <td>85</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>97.64268</td>\n",
              "      <td>98.252185</td>\n",
              "      <td>97.946484</td>\n",
              "      <td>787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>87.954545</td>\n",
              "      <td>86.143573</td>\n",
              "      <td>87.03964</td>\n",
              "      <td>3096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>14</td>\n",
              "      <td>94.605431</td>\n",
              "      <td>94.57096</td>\n",
              "      <td>94.588192</td>\n",
              "      <td>5191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>15</td>\n",
              "      <td>82.474227</td>\n",
              "      <td>71.301248</td>\n",
              "      <td>76.481836</td>\n",
              "      <td>800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>16</td>\n",
              "      <td>88.682363</td>\n",
              "      <td>89.383847</td>\n",
              "      <td>89.031723</td>\n",
              "      <td>2147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>17</td>\n",
              "      <td>97.959184</td>\n",
              "      <td>90.410959</td>\n",
              "      <td>94.033838</td>\n",
              "      <td>528</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>18</td>\n",
              "      <td>96.624473</td>\n",
              "      <td>99.134199</td>\n",
              "      <td>97.863248</td>\n",
              "      <td>1145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>19</td>\n",
              "      <td>97.076023</td>\n",
              "      <td>96.231884</td>\n",
              "      <td>96.652111</td>\n",
              "      <td>332</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>20</td>\n",
              "      <td>98.281787</td>\n",
              "      <td>96.949153</td>\n",
              "      <td>97.610922</td>\n",
              "      <td>286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>21</td>\n",
              "      <td>94.493542</td>\n",
              "      <td>91.749175</td>\n",
              "      <td>93.101139</td>\n",
              "      <td>1390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>22</td>\n",
              "      <td>84.672021</td>\n",
              "      <td>78.914535</td>\n",
              "      <td>81.69196</td>\n",
              "      <td>1265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>23</td>\n",
              "      <td>89.933775</td>\n",
              "      <td>96.243799</td>\n",
              "      <td>92.981856</td>\n",
              "      <td>1358</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>24</td>\n",
              "      <td>88.255034</td>\n",
              "      <td>86.229508</td>\n",
              "      <td>87.230514</td>\n",
              "      <td>789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>25</td>\n",
              "      <td>89.355742</td>\n",
              "      <td>77.239709</td>\n",
              "      <td>82.857143</td>\n",
              "      <td>319</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>26</td>\n",
              "      <td>89.502762</td>\n",
              "      <td>92.045455</td>\n",
              "      <td>90.756303</td>\n",
              "      <td>162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>27</td>\n",
              "      <td>90.990991</td>\n",
              "      <td>77.099237</td>\n",
              "      <td>83.471074</td>\n",
              "      <td>101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>29</td>\n",
              "      <td>93.072289</td>\n",
              "      <td>97.78481</td>\n",
              "      <td>95.37037</td>\n",
              "      <td>309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>30</td>\n",
              "      <td>77.777778</td>\n",
              "      <td>75.490196</td>\n",
              "      <td>76.616915</td>\n",
              "      <td>77</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>31</td>\n",
              "      <td>67.605634</td>\n",
              "      <td>93.203883</td>\n",
              "      <td>78.367347</td>\n",
              "      <td>96</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>32</td>\n",
              "      <td>83.035714</td>\n",
              "      <td>52.247191</td>\n",
              "      <td>64.137931</td>\n",
              "      <td>93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>33</td>\n",
              "      <td>85.714286</td>\n",
              "      <td>44.117647</td>\n",
              "      <td>58.252427</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>34</td>\n",
              "      <td>90.909091</td>\n",
              "      <td>92.52669</td>\n",
              "      <td>91.710758</td>\n",
              "      <td>520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>35</td>\n",
              "      <td>83.333333</td>\n",
              "      <td>55.555556</td>\n",
              "      <td>66.666667</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>36</td>\n",
              "      <td>100.0</td>\n",
              "      <td>62.5</td>\n",
              "      <td>76.923077</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>37</td>\n",
              "      <td>93.518519</td>\n",
              "      <td>99.019608</td>\n",
              "      <td>96.190476</td>\n",
              "      <td>101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>38</td>\n",
              "      <td>69.69697</td>\n",
              "      <td>58.974359</td>\n",
              "      <td>63.888889</td>\n",
              "      <td>23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>39</td>\n",
              "      <td>77.241379</td>\n",
              "      <td>80.0</td>\n",
              "      <td>78.596491</td>\n",
              "      <td>112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>40</td>\n",
              "      <td>100.0</td>\n",
              "      <td>99.642857</td>\n",
              "      <td>99.821109</td>\n",
              "      <td>279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>41</td>\n",
              "      <td>80.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>68.571429</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>42</td>\n",
              "      <td>100.0</td>\n",
              "      <td>64.705882</td>\n",
              "      <td>78.571429</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>43</td>\n",
              "      <td>-</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>45</td>\n",
              "      <td>-</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>46</td>\n",
              "      <td>-</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>accuracy=93.08</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-75605548-9291-4a06-a3d6-43f637b173a7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-75605548-9291-4a06-a3d6-43f637b173a7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-75605548-9291-4a06-a3d6-43f637b173a7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### #TODO 3\n",
        "Compare the result between all neural tagger models in 4.1.x and provide a convincing reason and example for the result of these models (which model perform better, why?)\n",
        "\n",
        "(If you use your own weight please state so in the answer)"
      ],
      "metadata": {
        "id": "Z6snY3s99Ylj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>Write your answer here :</b> <br>\n",
        "Accuracy ของโมเดลแรก (BiGru with CrossEntropyLoss) นั้นมีค่าเท่ากับ 90.49 <br>\n",
        "Accuracy ของโมเดลสอง (BiGru with CRF) นั้นมีค่าเท่ากับ 92.83 <br>\n",
        "Accuracy ของโมเดลสาม (BiGru with pretrained Embedding and CRF) นั้นมีค่าเท่ากับ 93.08 <br>\n",
        "จะเห็นว่าโมเดลที่สองมีคะแนนดีกว่าเพราะการใช้ CRF ทำให้โมเดลเห็นความสัมพันธ์ของคำรอบข้างมากขึ้น และโมเดลที่สามนั้นคะแนนดีที่สุดเพราะในชั้น embedding ที่ใช้ pretrained นั้นทำให้โมเดลเข้าใจความหมายของแต่ละ token ได้ดีกว่าสองโมเดลก่อนหน้า\n"
      ],
      "metadata": {
        "id": "RIaHr4qH9aqZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Z8raWOawjxp"
      },
      "source": [
        "### TODO 4\n",
        "\n",
        "Upon inference, the model also returns its transition matrix, which is learned during training. Your task is to observe and report whether the returned matrix is sensible. You can provide some examples to support your argument.\n",
        "\n",
        "#### **Hint** : The transition matrix must have the shape  of (num_class, num_class).\n",
        "##### **Write your answer here** จาก transition matrix จะเห็นว่าคะแนนของการต่อกันของ \"บรรลุ\"(VSTA) กับ \"วัตถุประสงค์\"(NCMN) มีค่า 0.085 ในขณะที่คะแนนของการต่อกันของ  \"วัตถุประสงค์\"(NCMN) กับ \"บรรลุ\"(VSTA) มีค่า 0.059 หมายความว่า transition matrix นั้นสมเหตุสมผลแล้ว"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ps_vl2HOv44v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8aa9d01d-8e97-4d7c-b2d9-40af42e8a86e"
      },
      "source": [
        "# INSERT YOUR CODE HERE IF NEEDED\n",
        "idx_to_label = {v:k for k,v in label_to_idx.items()}\n",
        "print('label_to_idx:\\n', label_to_idx)\n",
        "print()\n",
        "\n",
        "print('transition matrix:\\n', criterion_CRF_pretrained.transitions)\n",
        "print()"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label_to_idx:\n",
            " {'FIXN': 1, 'VACT': 2, 'NCMN': 3, 'PUNC': 4, 'CFQC': 5, 'DONM': 6, 'JCRG': 7, 'NCNM': 8, 'CNIT': 9, 'NPRP': 10, 'NTTL': 11, 'XVAM': 12, 'VSTA': 13, 'RPRE': 14, 'ADVN': 15, 'JSBR': 16, 'DDAC': 17, 'XVBM': 18, 'XVMM': 19, 'DIBQ': 20, 'PREL': 21, 'VATT': 22, 'XVAE': 23, 'DCNM': 24, 'CMTR': 25, 'FIXV': 26, 'PPRS': 27, 'XVBB': 28, 'DIAC': 29, 'PDMN': 30, 'DDAN': 31, 'CLTV': 32, 'ADVP': 33, 'NLBL': 34, 'ADVI': 35, 'CMTR@PUNC': 36, 'JCMP': 37, 'ADVS': 38, 'DDBQ': 39, 'NEG': 40, 'PNTR': 41, 'EITT': 42, 'DDAQ': 43, 'NONM': 44, 'EAFF': 45, 'DIAQ': 46, 'CVBL': 47}\n",
            "\n",
            "transition matrix:\n",
            " Parameter containing:\n",
            "tensor([[ 0.0996, -0.0638,  0.0298,  ..., -0.0597,  0.0878, -0.0530],\n",
            "        [ 0.0620,  0.0842, -0.0516,  ...,  0.0478, -0.0706, -0.0515],\n",
            "        [ 0.0333, -0.0524, -0.0739,  ..., -0.0048,  0.0614, -0.0261],\n",
            "        ...,\n",
            "        [-0.0615, -0.0587,  0.0685,  ..., -0.0978, -0.0429, -0.0337],\n",
            "        [-0.0540, -0.0957,  0.0096,  ..., -0.0417,  0.0515,  0.0089],\n",
            "        [ 0.0441, -0.0966,  0.0167,  ..., -0.0109, -0.0026,  0.0557]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def w2t(w1, w2):\n",
        "  return torch.LongTensor([ word_to_idx[w1], word_to_idx[w2] ])\n",
        "\n",
        "def t2tags(tokens):\n",
        "  model_BiGRU_CRF_pretrained.eval()\n",
        "  with torch.no_grad(): \n",
        "    tag_ids = model_BiGRU_CRF_pretrained(tokens.to(device)).cpu().detach().argmax(axis=1).numpy()\n",
        "    tags = [idx_to_label[t] for t in tag_ids]\n",
        "  return tags, tag_ids\n",
        "\n",
        "def llh(tag_id1, tag_id2):\n",
        "  return criterion_CRF_pretrained.transitions[tag_id1,tag_id2].cpu().detach().item()\n",
        "\n",
        "def get_POS_score(w1, w2):\n",
        "  tokens = w2t(w1, w2)\n",
        "  tags, tag_ids = t2tags(tokens)\n",
        "  print(tags)\n",
        "  l1, l2 = tag_ids\n",
        "  return llh(l1, l2)"
      ],
      "metadata": {
        "id": "DpmNGJh1VLPy"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_POS_score('บรรลุ', 'วัตถุประสงค์')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYDGgh4MYYkv",
        "outputId": "d17fb020-7784-4c50-c91d-292ad9f1cff5"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['VSTA', 'NCMN']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.08487164974212646"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "get_POS_score('วัตถุประสงค์', 'บรรลุ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1OFMYT9ZSNO",
        "outputId": "02023a6f-c1aa-431d-8a7f-9a4d0b35fe65"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NCMN', 'VSTA']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.05870155245065689"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SdhcdL3oZwT0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}